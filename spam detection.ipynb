{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam detection by Multinomial Naive Bayes classifier,  Logistic Regression, and SVM \n",
    "\n",
    "This project develops some algorithms to identify spam emails from non-spam emails in a dataset. I use multiple models (e.i., Naive Bayes classifier, Logistic Regression, and SVM). I applied multiple feature engineering technics (e.g., Count Vectorizer, Tfidf Vectorizer, and adding document length, non-word characters, etc) to improve accuracy. The highest spam detection accuracy **(AUC=98%)** is achieved by a multi-feature Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0\n",
       "5  FreeMsg Hey there darling it's been 3 week's n...       1\n",
       "6  Even my brother is not like to speak with me. ...       0\n",
       "7  As per your request 'Melle Melle (Oru Minnamin...       0\n",
       "8  WINNER!! As a valued network customer you have...       1\n",
       "9  Had your mobile 11 months or more? U R entitle...       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def read_data():\n",
    "    spam_data = pd.read_csv('spam.csv')\n",
    "    spam_data['target'] = np.where(spam_data['target']=='spam',1,0)\n",
    "    return(spam_data)\n",
    "\n",
    "spam_data = read_data()\n",
    "spam_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spam_data['text'], \n",
    "                                                    spam_data['target'], \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the percentage of spam emails in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.406317300789663"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data['target'].mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the training data `X_train` using a Count Vectorizer with default parameters. Then, finding the longest token in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'com1win150ppmx3age16subscription'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def count_vectorizer():\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    t = [len(w) for w in vect.get_feature_names()[:]] \n",
    "    return(vect.get_feature_names()[np.argmax(t)])\n",
    "\n",
    "count_vectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting and transforming the training data `X_train` using a Count Vectorizer with default parameters. Then, fitting a multinomial Naive Bayes classifier model with smoothing `alpha=0.1`. Finally, finding the area under the curve (AUC) score using the transformed test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9720812182741116"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def multinomial_naive_bayes_classifier():\n",
    "    vect = CountVectorizer().fit(X_train)\n",
    "    x_train_vectorized = vect.transform(X_train)\n",
    "    classify = MultinomialNB(alpha=0.1)\n",
    "    classify.fit(x_train_vectorized, y_train)\n",
    "    predicted_lables = classify.predict(vect.transform(X_test))    \n",
    "    return (roc_auc_score(y_test,predicted_lables))\n",
    "\n",
    "multinomial_Naive_Bayes_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting and transforming the training data `X_train` using a Tfidf Vectorizer with default parameters. Then, finding the 20 features which have the smallest tf-idf and 20 features which have the largest tf-idf. Then, the features are sorted in a two series where each series is sorted by tf-idf value and then alphabetically by feature name. The index of the series is the feature name, and the data should be the tf-idf.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(aaniye          0.074475\n",
       " athletic        0.074475\n",
       " chef            0.074475\n",
       " companion       0.074475\n",
       " courageous      0.074475\n",
       " dependable      0.074475\n",
       " determined      0.074475\n",
       " exterminator    0.074475\n",
       " healer          0.074475\n",
       " listener        0.074475\n",
       " organizer       0.074475\n",
       " pest            0.074475\n",
       " psychiatrist    0.074475\n",
       " psychologist    0.074475\n",
       " pudunga         0.074475\n",
       " stylist         0.074475\n",
       " sympathetic     0.074475\n",
       " venaam          0.074475\n",
       " diwali          0.091250\n",
       " mornings        0.091250\n",
       " dtype: float64,\n",
       " 146tf150p    1.000000\n",
       " 645          1.000000\n",
       " anything     1.000000\n",
       " anytime      1.000000\n",
       " beerage      1.000000\n",
       " done         1.000000\n",
       " er           1.000000\n",
       " havent       1.000000\n",
       " home         1.000000\n",
       " lei          1.000000\n",
       " nite         1.000000\n",
       " ok           1.000000\n",
       " okie         1.000000\n",
       " thank        1.000000\n",
       " thanx        1.000000\n",
       " too          1.000000\n",
       " where        1.000000\n",
       " yup          1.000000\n",
       " tick         0.980166\n",
       " blank        0.932702\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def small_large_tf_idf():\n",
    "    vect = TfidfVectorizer().fit(X_train)\n",
    "    features = np.array(vect.get_feature_names())\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    sorted_features = X_train_vectorized.max(0).toarray()[0].argsort()\n",
    "    values = X_train_vectorized.max(0).toarray()[0]\n",
    "    smalls  = pd.Series(values[sorted_features[:20]], index = features[sorted_features[:20]])\n",
    "    larges  = pd.Series(values[sorted_features[:-21:-1]], index = features[sorted_features[:-21:-1]])\n",
    "    smalls = smalls.reset_index()\n",
    "    smalls = smalls.sort_values([0, 'index'])\n",
    "    larges= larges.reset_index()\n",
    "    larges = larges.sort_values([0, 'index'], ascending=[False, True])\n",
    "    return (pd.Series(np.array (smalls[0]), index = np.array(smalls['index'])), \n",
    "            pd.Series(np.array (larges[0]), index = np.array(larges['index'])))\n",
    "\n",
    "small_large_tf_idf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use Tfidf Vectorizer to fit and transform the training data `X_train`. Tfidf Vectorizer ignors terms that have a document frequency strictly lower than **3**. Then I fitted a multinomial Naive Bayes classifier model with smoothing `alpha=0.1` and compute the area under the curve (AUC) score using the transformed test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9416243654822335"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multinomial_naive_bayes_classifier():\n",
    "    vect = TfidfVectorizer(min_df=3).fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    classifier = MultinomialNB(alpha=0.1)\n",
    "    model = classifier.fit(X_train_vectorized, y_train)\n",
    "    predicted_lables = model.predict(vect.transform(X_test))    \n",
    "    return (roc_auc_score(y_test,predicted_lables))\n",
    "\n",
    "multinomial_Naive_Bayes_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the average length of documents (number of characters) for not spam and spam documents, to find some insightful new features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71.02362694300518, 138.8661311914324)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def length_of_emails():\n",
    "    spams= spam_data[spam_data['target']== 1]\n",
    "    normal = spam_data[spam_data['target']== 0]    \n",
    "    return (np.array([len(w) for w in normal['text']]).mean(), np.array([len(w) for w in spams['text']]).mean())\n",
    "\n",
    "length_of_emails()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "The following function has been provided to combine new features into the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "def add_feature(X, feature_to_add):\n",
    "\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a Tfidf Vectorizer to fit and transform the training data X_train. Tfidf Vectorizer ignores terms that have a document frequency strictly lower than **5**. Then, I use this document-term matrix and an additional feature, **the length of document (number of characters)**, to fit a Support Vector Classification model with regularization `C=10000`. Finally, the area under the curve (AUC) score is calculated for the transformed test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9661689557407943"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def sv_classifier():\n",
    "    vect = TfidfVectorizer(min_df=5).fit(X_train)\n",
    "    X_train_vectorized = vect.transform(X_train)\n",
    "    X_train_new_feature = add_feature(X_train_vectorized, X_train.str.len())\n",
    "    classifier = SVC(C=10000)\n",
    "    model = classifier.fit(X_train_new_feature, y_train)\n",
    "    X_test_vectorized = vect.transform(X_test)\n",
    "    predicted_lables = model.predict(add_feature(X_test_vectorized, X_test.str.len()))    \n",
    "    return (roc_auc_score(y_test,predicted_lables))\n",
    "\n",
    "SV_Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the average number of digits per document for not spam and spam documents, to find some insightful new features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2992746113989637, 15.759036144578314)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_digits():\n",
    "    spams = spam_data[spam_data['target']== 1]\n",
    "    normal = spam_data[spam_data['target']== 0]    \n",
    "    return(np.array([sum(c.isdigit() for c in document) for document in normal['text']]).mean(), \n",
    "            np.array([sum(c.isdigit() for c in document) for document in spams['text']]).mean()) \n",
    "\n",
    "count_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I fit and transform the training data `X_train` using a Tfidf Vectorizer. The Tfidf Vectorizer ignores terms that have a document frequency strictly lower than **5** and using **word n-grams from n=1 to n=3** (unigrams, bigrams, and trigrams). I also added two new features to document-term matrix:\n",
    "\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "\n",
    "Then, I fit a Logistic Regression model with regularization `C=100` and compute the area under the curve (AUC) score using the transformed test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sertab Gamma\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9809793219360643"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression_model():\n",
    "vect = TfidfVectorizer(min_df=5, ngram_range=(1,3)).fit(X_train)\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_new_feature = add_feature(X_train_vectorized, X_train.str.len())\n",
    "X_train_new_feature = add_feature(X_train_new_feature, \n",
    "                                  [sum(c.isdigit() for c in document) for document in X_train])\n",
    "\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "X_test_new_feature = add_feature(X_test_vectorized, X_test.str.len())\n",
    "X_test_new_feature = add_feature(X_test_new_feature, \n",
    "                                  [sum(c.isdigit() for c in document) for document in X_test])\n",
    "\n",
    "classifier = LogisticRegression(C=100)\n",
    "model = classifier.fit(X_train_new_feature, y_train)\n",
    "predicted_lables = model.predict(X_test_new_feature)    \n",
    "roc_auc_score(y_test,predicted_lables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the average number of non-word characters (anything other than a letter, digit or underscore) per document for not spam and spam documents, to find some good new features. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17.29181347150259, 29.041499330655956)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "spams = spam_data[spam_data['target']== 1]\n",
    "normal = spam_data[spam_data['target']== 0]   \n",
    "(np.array([len(re.findall(r'\\W', document)) for document in normal['text']]).mean(),\n",
    "         np.array([len(re.findall(r'\\W', document)) for document in spams['text']]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a Count Vectorizer to fit and transform the training data X_train. The Count Vectorizer ignores terms that have a document frequency strictly lower than **5** and using **character n-grams from n=2 to n=5.**\n",
    "\n",
    "Character n-grams creates character n-grams only from text inside word boundaries. This should make the model more robust to spelling mistakes.\n",
    "\n",
    "Then, I add three new features to the document-term matrix:\n",
    "* the length of document (number of characters)\n",
    "* number of digits per document\n",
    "* number of non-word characters (anything other than a letter, digit or underscore.)\n",
    "\n",
    "I used a Logistic Regression model with regularization C=100 and compute the area under the curve (AUC) score using the transformed test data.\n",
    "\n",
    "Finally, I list the 10 smallest and 10 largest coefficients from the model and return them along with the AUC score in a tuple. \n",
    "\n",
    "This code returns a tuple `(AUC score as a float, smallest coefs list, largest coefs list)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sertab Gamma\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9813973821367333,\n",
       " ..    -1.403017\n",
       " .     -1.193355\n",
       "  i    -0.814625\n",
       "  go   -0.735068\n",
       " ?     -0.702470\n",
       "  y    -0.700125\n",
       " pe    -0.654805\n",
       " go    -0.641928\n",
       " ok    -0.625421\n",
       " h     -0.621703\n",
       " dtype: float64,\n",
       " ne     1.496931\n",
       " co     0.743351\n",
       " ww     0.733814\n",
       " ia     0.725831\n",
       " xt     0.638078\n",
       " ar     0.629200\n",
       "  ch    0.626533\n",
       " mob    0.608416\n",
       " uk     0.588750\n",
       "  a     0.560133\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, ngram_range=(2,5), analyzer= 'char_wb').fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_new_feature = add_feature(X_train_vectorized, X_train.str.len())\n",
    "X_train_new_feature = add_feature(X_train_new_feature, \n",
    "                                  [sum(c.isdigit() for c in document) for document in X_train])\n",
    "X_train_new_feature = add_feature(X_train_new_feature, \n",
    "                                  [len(re.findall(r'\\W', document)) for document in X_train])\n",
    "\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "X_test_new_feature = add_feature(X_test_vectorized, X_test.str.len())\n",
    "X_test_new_feature = add_feature(X_test_new_feature, \n",
    "                                  [sum(c.isdigit() for c in document) for document in X_test])\n",
    "X_test_new_feature = add_feature(X_test_new_feature, \n",
    "                                  [len(re.findall(r'\\W', document)) for document in X_test])\n",
    "\n",
    "classifier = LogisticRegression(C=100)\n",
    "model = classifier.fit(X_train_new_feature, y_train)\n",
    "\n",
    "predicted_lables = model.predict(X_test_new_feature)    \n",
    "\n",
    "\n",
    "t = classifier.coef_[0].argsort()\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "(roc_auc_score(y_test,predicted_lables) ,\n",
    "     pd.Series([classifier.coef_[0][t[i]] for i in range (10)], index =[feature_names[t[i]] for i in range (10)]) ,\n",
    "     pd.Series([classifier.coef_[0][t[-i-1]] for i in range (10)], index =[feature_names[t[-i-2]] for i in range (10)])\n",
    "    )   \n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "Pn19K",
   "launcher_item_id": "y1juS",
   "part_id": "ctlgo"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
